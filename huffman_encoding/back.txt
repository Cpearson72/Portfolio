\chapter{Introduction}

\label{chapter:introduction}

\section{Proteins}

Proteins are the molecular machines that are essential to the process of life.
For example, transmembrane proteins allow molecules to move into and out of the 
cell.
Hemoglobin ferries iron through the blood, while immunoglobulin provides
for defense against pathogens.
Actin contracts our muscles, and myelin insulates our nerves.

It is well known that DNA encodes the genetic information that determines how
we develop and function.
Portions of this DNA are transcribed into RNA, and then 
a complex piece of cellular machinery called the ribosome translates this RNA 
into amino acids, the building blocks of proteins.
Proteins are the machines for which the
DNA is the blueprint.
Chains of amino acids fold into intricate, low-energy
forms, and these structures {\emph do things}.

It is the structure of
a protein that allows it to perform its function, and while this structure 
is determined by the
amino acid sequence that derives from DNA, the relationship between sequence
and structure is not simple.

% During the course of evolution, mutations cause changes in protein sequences.
% Some of these mutations cause harm, and are selected against, while others
% provide benefits, and are selected for.
% Some of these mutations will have
% little effect on the structure of the protein, while others may dramatically
% alter the structure, and thus the function.


\subsection{Primary Structure}

Proteins are composed of linear chains of molecules called \textit{amino acids}.
An amino acid is a molecule comprising an \textit{amine} group, a 
\textit{carboxyl} group, and one of twenty possible \textit{sidechains} (see
Figure~\ref{amino_acid}).
Each of these components is attached to a carbon atom, known as the 
$\alpha$-carbon.

\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=5in]{intro/amino-acid.pdf}}
   \caption[The general structure of  an amino acid showing the hydrogen(H), 
   nitrogen(N), oxygen(O) and carbon(C,$\text{C}_{\alpha}$) atoms.]{The general 
   structure of  an amino acid showing the hydrogen(H), nitrogen(N), oxygen(O) 
   and carbon(C,$\text{C}_{\alpha}$) atoms.
   The sidechain is one of twenty possible ``decorations;'' amino acids differ
   only in their sidechains. }
   \label{amino_acid}
 \end{center}
\end{figure}


Amino acids bind to one another via a \textit{peptide bond}, which forms when
the carboxyl group of one amino acid gives up an oxygen and hydrogen to bind
with the amine group of another amino acid, which gives up a hydrogen. 
This results in a free water molecule. 
In addition, as multiple amino acids form \textit{polypeptide chains}, the 
unbound amine group at one end is known as the \textit{N-terminal} end of the
resulting protein, while the unbound carboxyl group at the other end is known
as the \textit{C-terminal} end (Figure~\ref{polypeptide}).

\begin{figure}[htb!]
\advance\leftskip-0.3in
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/peptide.pdf}}
   \caption[Peptide bonds and the protein backbone]{Amino acids join by peptide 
   bond to form the backbone.  
   Individual amino acids are highlighted with ovals.
   The sidechain connected to the $\text{C}_{\alpha}$  is different for each 
   amino acid. 
    When a peptide bond forms, a water molecule forms from the hydrogen given
    up by the nitrogen end of one amino acid, and the oxygen and hydrogen given
    up by the carbon end of the other.}
   \label{polypeptide}
 \end{center}
\end{figure}

The peptide linkages, along with the $\alpha$-carbon atoms, form the 
\textit{backbone} of the protein.
Ultimately, the protein folds into a globular form, generally representing a
lowest-energy conformation.
It is useful to describe the structure of proteins at several levels of
organization.

The \textit{primary structure} of a protein is simply its sequence of amino
acids.
In principal, any of the 20 standard amino acids can occur in any
position of an amino acid chain; for a protein of length $n$, there are $20^{n}$
possible protein sequences.
Of course, the subset of those sequences that will fold into a compact, 
three-dimensional structure is much smaller; the subset of \emph{those} that 
would fold into a compact, three-dimensional structure that exists in nature is
smaller still.
However, determining which protein sequences nature allows is not 
trivial.

\subsection{Secondary Structure}

Local interactions among amine and carbonyl groups result in \textit{hydrogen
bonds} between amino acids that are not immediately adjacent in sequence.
A hydrogen bond is the electrostatic attraction between a hydrogen atom in
one amino acid and an oxygen or nitrogen atom in another.
In particular, we can describe the \textit{secondary structure} of a protein
according to the shape of the angles of the backbone.
The most common type of secondary structure is the $\alpha$-helix, in which the
protein backbone coils into a twisted shape, stabilized by hydrogen bonds 
(Figure~\ref{helix}).
The most common $\alpha$-helices have hydrogen bonds between residues
four positions apart in sequence.
Other, less common helical structures include the $3_{10}$ helix, in which
residues three apart in sequence form hydrogen bonds, and the $\pi$ helix, in 
which residues five apart in sequence form hydrogen bonds.

Another secondary structure is the $\beta$-strand, which in combination form 
$\beta$-sheets.
$\beta$-strands occur when the backbone is stretched out; typically, this 
conformation is stabilized by hydrogen bonds between adjacent strands
(Figure~\ref{sheet}), resulting in $\beta$-sheets.
The hydrogen bonds in $\beta$-sheets may occur between residues that are very
far apart from each other in the amino acid sequence.
$\beta$-strands in a sheet may be parallel or anti-parallel to one another with
respect to the direction of the amino acid sequence.

The remainder of local backbone conformations, consisting of turns, bulges,
loops, bridges, etc., have been classified into several different subcategories,
but is often grouped together into a third category of secondary structure,
commonly referred to as a \emph{coil}.

\begin{figure}[htb!]
\advance\leftskip-0.3in
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/helix.pdf}}
   \caption[$\alpha$-helix secondary structure.]{$\alpha$-helix secondary 
   structure.
   Hydrogen bonds between residues 4 positions apart in sequence cause the
   helical shape.
   Other, less common helix structures include the $3_{10}$ helix, in which
   residues 3 apart in sequence form hydrogen bonds, and the $\pi$ helix,
   in which residues 5 apart in sequence form hydrogen bonds.}
   \label{helix}
 \end{center}
\end{figure}

\begin{figure}[htb!]
\advance\leftskip-0.3in
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/sheet.pdf}}
   \caption[$\beta$-sheet secondary structure.]{$\beta$-sheet secondary 
   structure.
   Hydrogen bonds between residues that may be quite far apart in sequence
   cause this pleated, sheet-like shape.
   Antiparallel $\beta$-strands are shown here; parallel $\beta$-strands also 
   exist.
   }
   \label{sheet}
 \end{center}
\end{figure}


\subsection{Supersecondary and Tertiary Structure}

We can mark secondary structural elements of the complete structure of a protein
backbone as it is folded in three-dimensional space, and consider the pattern
of where the $\alpha$-helices and $\beta$-strands lie.
For example, $\beta$-strands can be organized into $\beta$-barrels 
(Figure~\ref{barwin}), sandwiches,
or propellers; $\alpha$-helices can be organized into 2- or 4-helix bundles, and
there are other patterns of strand topologies that involve mixed collections of
$\alpha$-helices and $\beta$-strands.
The topologies of the various strand positions are known as 
\emph{super-secondary structure}.

\begin{figure}[ht!]
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/barwin.pdf}}
   \caption[Super-secondary structure ``cartoon'' of Barwin (PDB ID 
   1BW3)]{Super-secondary structure ``cartoon'' of Barwin (PDB ID 1BW3).
   Barwin, an endoglucanase, has eight $\beta$-strands forming a closed 
   ``barrel''
   shape, as well as four $\alpha$-helices.}
   \label{barwin}
 \end{center}
\end{figure}

The \textit{tertiary structure} of a protein is the fully-specified
three\--dimensional position of every atom.
The orientation of the backbone atoms in three-dimensional space forms
three distinguishing dihedral angles:
$\phi$ between the carbon-1-nitrogen and $\alpha$-carbon-carbon-1
atoms in an amino acid, $\psi$ between the nitrogen-$\alpha$-carbon and carbon-1-nitrogen atoms,
and $\omega$ between the $\alpha$-carbon-carbon-1 and the nitrogen and 
$\alpha$-carbon of the next amino acid (see
Figure~\ref{angles}).
The angle $\omega$ is usually $0^\circ$, and occasionally $180^\circ$.
The
side chain of each amino acid must then pack into a low-energy state
in such a way that it does not interfere with the other amino acids in the
protein.
The tertiary structure represents, in most cases, a global minimum energy state,
also known as the \textit{native state}.
Many proteins have now had their tertiary structure determined by X-ray 
crystallography, or by nuclear magnetic resonance (NMR) spectroscopy.
A protein whose structure has been determined experimentally is said to have a
\emph{solved} structure.
However, the difficulty of experimentally solving the structure of any 
particular protein of interest can vary.
X-ray crystallography's limiting factor is that not all proteins can be
put into solution and crystallized, while NMR's limiting factor is primarily 
computational.
The Protein Data Bank (PDB)~\cite{Bernstein:1977un, Berman:2000hl} is a publicly available 
database that contains the atomic coordinates of all proteins whose tertiary 
structure has been solved.

\begin{figure}[ht!]
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/angles.pdf}}
   \caption[Backbone angles]{The orientation of the backbone atoms in 
   three-dimensional space 
   forms three dihedral angles:
     $\phi$ between the carbon-1-nitrogen and $\alpha$-carbon-carbon-1
    atoms, $\psi$ between the nitrogen-$\alpha$-carbon and carbon-1-nitrogen 
    atoms,
    and $\omega$ between the $\alpha$-carbon-carbon-1 and the nitrogen and 
    $\alpha$-carbon of the next amino acid.}
   \label{angles}
 \end{center}
\end{figure}

Finally, \textit{quaternary structure} describes how multiple tertiary 
structures interact; these may be multiple duplicate protein chains (for 
example, a homodimer is a complex of two identical protein chains, while a 
heterodimer is a complex comprising two different protein chains).
In this work, we focus on individual chains, rather than quaternary structures.

\subsection{Protein Data Sets}


In order to make sense of the evolutionary, structural, and functional 
relationships among proteins, biologists have created several organizational
schemes.
Structural Classification Of Proteins 
(SCOP)~\cite{Murzin:1995uh, Andreeva:2004ic}
and CATH (which stands for Class, Architecture, Topology,
and Homologous superfamily)~\cite{Orengo:1997vy, Pearl:2003wb, Greene:2007iu}
are hierarchical schemes that
place proteins in a tree based primarily on structural, but also on evolutionary
and functional similarities.
In this work, we will primarily rely on SCOP, since it has been used in many
homology detection 
studies~\cite{Elofsson:1999tj, Wistrand:2004ia, Soding:2005ff}.

SCOP organizes all protein sequences of known structure (with some time delay)
into a four-level hierarchy.
The top level of the SCOP hierarchy is \emph{class}, which distinguishes the
primary secondary-structural composition of proteins: mainly-$\alpha$,
mainly-$\beta$, mixed $\alpha$ and $\beta$, cellular-membrane proteins, among
others.
The second level of the SCOP hierarchy is \emph{fold}, which organizes proteins
by overall structural motif, or supersecondary structure.
Proteins in the same fold are not necessarily evolutionarily related.
Below fold is \emph{superfamily}, which organizes proteins that share
evolutionary relationships, as well as similar structure and function.
Below the superfamily level is the \emph{family} level of SCOP.
Proteins in the same family have clear evolutionary relationships, and a
significant level of sequence similarity.
Figure~\ref{scop-hierarchy} illustrates the SCOP hierarchy.

\begin{figure}[htb!]
\advance\leftskip-0.3in
\begin{center}
  \fbox{\includegraphics[width=5in] {intro/scop-hierarchy.pdf}}
   \caption[The SCOP hierarchy of protein structure.]{The SCOP hierarchy of 
   protein structure.
   \emph{Class} organizes proteins in large part according to supersecondary
   structural content.
   \emph{Fold} organizes proteins by supersecondary structural motifs.
   \emph{Superfamily} organizes proteins by structural, functional, and
   evolutionary similarity, while \emph{Family} organizes them by sequence
   similarity as well.
   }
   \label{scop-hierarchy}
 \end{center}
\end{figure}

\subsection{Protein Folding}

The process by which a protein, as its amino acid sequence is emitted by the
ribosome, forms its stable tertiary structure is called \textit{folding}.
In 1969, molecular biologist Cyrus Levinthal noted~\cite{Levinthal:1969vj} that 
even given a coarse
(tripartite) discretization of bond angles, a protein of merely 100 amino acids
(a short chain by most standards) could take $3^{300}$ distinct 
three-dimensional conformations (tertiary structures).
Given the accepted view that proteins
typically fold to globally minimum energy states, Levinthal noted that a
protein would take the lifetime of the universe to find a minimum energy state
by sampling the entire fold space.
However, in practice, proteins fold in
microseconds or milliseconds.
This apparent paradox became known as Levinthal's
Paradox; the solution to the paradox must be that nature does not explore the
entire fold space.

The rapidity of protein folding is thought to be because proteins fold along
\textit{folding funnels}, which prune much of the 
possible fold space very quickly~\cite{Dill:1997vn, Matagne:1998uu, Tsai:1999we, Dinner:2000vu}. 
It is even conjectured~\cite{Rost:2002wx,Cuff:2009cz} that only those proteins that exhibit fold 
funnels that allow them to fold quickly have evolved; protein sequences that
would not quickly find stable native states would be selected against during
the course of evolution.

Physics-based approaches to computationally solving the protein folding problem 
try to solve the various force field equations 
(including hydrophobic, electrostatic, and van der Waals forces) to find a
minimum-energy state~\cite{BornbergBauer:1999wu, Heun:1999wr}.
However, even the most simplified models can prove computationally intractable.
In 1998, Berger and Leighton~\cite{Berger:1998vv} proved that the seemingly simple
HP (hydrophobic-hydrophilic) lattice model of protein folding is NP-hard.

For some purposes, such as understanding the molecular motion of proteins such
as ion channels (which control the flow of ions through a cellular membrane) or
flagellin (which forms the moving filament in bacterial flagella), just knowing 
the native state is not enough, and molecular dynamics simulations are 
necessary.

The current state of the art in full tertiary structure prediction via
molecular dynamics modeling relies on huge computational infrastructures; we 
describe two of them.
The first, Anton, is a
supercomputer purpose-built for protein simulations by the D.E.
Shaw Research~\cite{Shaw:2007cr}.
The second, Folding@home, is a worldwide distributed-computing system developed
at Stanford~\cite{Jayachandran:2006ur}; 
it uses spare CPU and GPU cycles on desktop, laptop, and video
game systems around the world.
Both of these systems can compute a few
milliseconds of simulation time per day.

Fortunately, however, it is not always necessary to determine tertiary 
structure to the level of precision achieved by experimental methods.
Computational biology methods that use statistical energy functions and
secondary or supersecondary structure prediction have made significant progress
in the last ten years~\cite{Moult:2006tn}.
In particular, \emph{approximately} predicting the tertiary structure--or
predicting the supersecondary structure--may be adequate when the end goal
is function prediction or homology detection.


\section{Protein Homology}

An alternative to experimentally predicting the structure of a protein is to try
to determine, based on \emph{sequence} similarity, that a protein of interest
is sufficiently closely related, in evolutionary terms, to some other protein
of solved structure that it is likely to fold into a similar shape.
However, while the task gets easier the closer it becomes to that of 
determining
sequence, the quality of the results worsens; protein sequence is less well
\textit{conserved} than structure; that is, fairly significantly different
protein sequences may nonetheless share quite similar structures and functions~\cite{DunbrackJr:2006dx}.

Biologists say that two proteins are \emph{homologous} when they are derived
from a common ancestor.
Often, homologous proteins share common structure.
When two protein sequences are similar, it is relatively easy to determine that
they are homologous.
However, homologous proteins may differ significantly in terms of sequence
identity.
Sequence analysis methods have long allowed for the detection of homologous
proteins, provided sequence divergence is not too great.
The problem of detecting homologous proteins when sequence similarity is low is
known as \emph{remote} homolog detection.
The purpose of this thesis is to develop novel methods for remote
homology detection.
Now, we will survey existing methods for homology detection.

% \subsection{Substitution Matrices}
% 
% Sequence analysis methods have long allowed for the detection of homologous
% proteins, provided sequence divergence is not too great.
% The Point Accepted Mutation (PAM) substitution matrix~\cite{} is based on the
% relative mutability of different amino acids, as well as their frequency of
% occurrence.
% The relative mutability of amino acids is determined by examining 
% \emph{alignments} of sequences known to be homologous, and observing the
% frequency of one particular amino acid having mutated to another at a particular
% position in the alignment.
% 
% Another commonly used substitution matrix, commonly used by sequence alignment
% programs, is BLOcks of amino acid SUbstitution Matrix (BLOSUM)~\cite{HH92}.
% The BLOSUM matrix is a diagonally symmetric matrix, with an entry containing
% a score for each possible pair of amino acids.
% The score $s(i,j)$ for two residues $i$ and $j$ is given by:
% \begin{equation}
%   s(i,j) = \frac{log \frac{P_{i,j}}{f_{i}f_{j}}}{\lambda}
% \end{equation}
% where $P_{i,j}$ is the probability of observing residues $i$ and $j$ aligned
% in homologous sequences, and $f_{i}$ is the observed background frequency of
% residue $i$, and $\lambda$ is a scaling factor chosen to produce integer
% values for the scores~\cite{}.
% Different variants of the BLOSUM matrices exist; for a chosen threshold $L$,
% only sequences within a sequence identity threshold of $L\%$ are clustered into
% a single representative sequence; those sequences are then aligned and the
% alignment used to compute the BLOSUM$L$ matrix.
% Thus, BLOSUM80 is intended for use in less divergent sequence alignments,
% while BLOSUM50 is intended for use in more divergent sequence alignments.
% BLOSUM62 is a commonly used default scoring matrix for protein sequence
% alignment tools such as BLAST~\cite{}.
% 
% As a matter of historical interest, the BLOSUM62 matrix used for many years is 
% not faithful to the algorithm of Henikoff and Henikoff, but the miscalculated 
% matrix seems to improve performance of homology search~\cite{Styczynski08}.
% 
% \subsection{Sequence Alignment}
% 
% Given a scoring matrix that can be used to compute the similarity of two amino
% acids, it is reasonably straightforward to compute the similarity of two
% proteins of the same length, if we simply put each pair of amino acids into
% correspondence.
% Given sequences $a$ and $b$, each of which is a sequence of amino acids
% of length $n$, then we can simply compute an alignment score as follows:
% \begin{equation}
%   s(a,b) = \frac{\sum_{i=1}^{n} H(a_{i},b_{i})}{n}
% \end{equation}
% where $a_{i}$ and $b_{i}$ are the $i$th amino acid of each sequence, and
% $H(a_{i},b_{i})$ is the score for that pair of residues in a scoring matrix of
% choice (such as BLOSUM62).
% Thus, matching amino acids will obtain high scores, and mismatching amino acids
% will obtain lower scores, depending on the substitution matrix used.
% 
% Clearly, this kind of alignment algorithm contains a significant flaw: during
% the course of evolution, amino acids are not simply substituted for one another,
% but may also be inserted or deleted.
% Consider the protein sequences \texttt{ALVLNQY} and \texttt{AILVLNQ}.
% While these are the same length, it is apparent that the second sequence has
% had an isoleucine (\texttt{I}) inserted before the subsequence \texttt{LVLLQ},
% and is missing the tyrosine (\texttt{Y}) at the end.
% Our simplistic approach would not recognize this possibility, and would score
% quite poorly, as only the leading alanine (\texttt{A}) would score an exact
% match.
% Because of this, protein sequences are more often compared using \emph{gapped}
% alignment, which recognizes insertions and deletions as well as substitutions.
% 
% A gapped alignment represents positions at which one protein has no amino acid
% as gaps, often visually displayed with a `-' character.
% A gapped alignment between our two example protein sequences might match
% \texttt{A-LVLNQY} with \texttt{AILVLNQ-}, which aligns matching residues by
% representing the insertion and deletion.
% 
% \subsubsection{Gapped Alignment} \label{gaps}
% 
% Two algorithms for gapped alignment are the Needleman-Wunsch~\cite{} algorithm,
% which performs \emph{global} alignment (which means that two sequences are
% aligned in their entirety, from end to end), and the Smith-Waterman~\cite{}
% algorithm, which performs \emph{local} alignment.
% A local alignment does not force the entirety of both sequences into alignment;
% it aligns a high-scoring pair of \emph{subsequences}, omitting any
% poorly-scoring (presumably not evolutionarily conserved) residues from either
% end.
% We will explain the Smith-Waterman algorithm, because local alignments are
% more useful for protein homology search~\cite{Altschul}.
% 
% The Smith-Waterman algorithm comprises a set of recurrence relations, which are
% traditionally solved using dynamic programming:
% 
% A matrix $H$ is constructed such that $H(i,0) = 0,\; 0\le i\le m$ and
% $H(0,j) = 0,\; 0\le j\le n$, where $a, b$ are strings over the alphabet of
% amino acids and $m, n$ are the length of $a, b$ respectively.
% 
% \begin{equation} 
%   \begin{split} 
%   H(i,j) = \max \begin{Bmatrix}
%   0  \\
%   H(i-1,j-1) + \ w(a_i,b_j) & \text{Match/Mismatch} \\
%   H(i-1,j) + \ w(a_i,-) & \text{Deletion} \\
%   H(i,j-1) + \ w(-,b_j) & \text{Insertion}
%   \end{Bmatrix}
%   ,\;\\ 1\le i\le m, 1\le j\le n\\
% \text{ where }
%   w(a_i, b_j) = \text{ lookup in scoring matrix such as BLOSUM62}
%   \end{split}
% \end{equation}
% 
% Note that the scoring matrix must be expanded to include the gap state;
% and there are further refinements~\cite{Gotoh} for affine gap penalties that are
% used in modern implementations.
% 
% With proper choices of gap penalties, the Smith-Waterman method produces
% alignments that are biologically useful.
% The chief limitation of Smith-Waterman is that it relies on dynamic programming,
% and its asymptotic complexity is $O(mn)$ where $m$ and $n$ are the lengths of
% the sequences being aligned.
% As sequence lengths grow, or when many pairwise searches must be done (such as
% in \emph{multiple alignment}, in which multiple sequences are put into
% correspondence), this becomes a limitation.
% In particular, for the purpose of homology search, as opposed to aligning two
% sequences of interest, a query sequence must be aligned against thousands or
% millions of potentially homologous sequences.

\subsubsection{BLAST}

Altschul, et al. developed the Basic Local Alignment Search Tool (BLAST)~\cite{Altschul:1990dw}
algorithm as a faster alternative to dynamic programming-based methods such
as the Smith-Waterman~\cite{Smith:1981up} algorithm.
BLAST uses a number of heuristics to reduce the time required to perform an
alignment, at the possible expense of some accuracy.
BLAST also relies on an indexed database of sequences to be searched.
% The BLAST search algorithm is as follows:
% \begin{enumerate}
%   \item Remove low-complexity regions in the query sequence.
%   These are subsequences that contain single or tandem repeats, or a low
%   diversity of amino acids.
%   \item Make a list of $k$-tuples from the query sequence.
%   A typical choice of $k$ might be 3, in which case every trigram of the query
%   sequence would be indexed.
%   \item For each $k$-tuple $K$:
%   \begin{enumerate} 
%     \item Enumerate all other possible $k-tuples$.
%     For $k=3$, there are $20^{3}$ possible tuples.
%     \item For each possible $k-tuple$ $P$, use the substitution matrix of choice
%     (such as BLOSUM62) to score it against $K$.
%     \item If the score is better than some threshold $T$, organize $P$ into a 
%     search tree.
%   \end{enumerate}
%   
%   \item For each $k$-tuple $K$ \emph{and} its associated high-scoring tuples 
%         $P$:
%   \begin{enumerate}
%     \item Locate the identically matching $k$-tuples in the database.
%     \item Each match is used as a \emph{seed} for a possible ungapped extension
%     in the database.
%     Such an extension is called a High-scoring Segment Pair (HSP).
%     \item Each HSP is extended to the left and right until the resulting
%     alignment score drops below some predetermined threshold.
%     \item Store the HSP in a list.
%   \end{enumerate}
%   \item For each HSP found, determine if it is statistically significant.
%   Altschul proved~\cite{} that the alignment scores of two randomly chosen
%   sequences follow the Gumbel~\cite{} Extreme Value Distribution (EVD).
%   Discard HSPs that are not statistically significant, below some predetermined
%   threshold.
%   \item Attempt to combine multiple HSPs within a single database sequence into
%   a longer HSP.
%   \item Finally, re-align each HSP to the query sequence using the
%   Smith-Waterman algorithm, and return the resulting alignments and their
%   scores.
% \end{enumerate}

BLAST allows for fast search through databases to find potential homologs.
The protein-specific version of BLAST is called BLASTP.
BLASTP uses a \emph{substitution matrix} to score alignments; the most commonly
used substitution matrix is BLOcks of amino acid SUbstitution Matrix
(BLOSUM)~\cite{Henikoff:1992tk}.
A BLOSUM score $s(i,j)$ for two residues $i$ and $j$ is given by:
\begin{equation}
  s(i,j) = \frac{log \frac{P_{i,j}}{f_{i}f_{j}}}{\lambda}
\end{equation}
where $P_{i,j}$ is the probability of observing residues $i$ and $j$ aligned
in homologous sequences, and $f_{i}$ is the observed background frequency of
residue $i$, and $\lambda$ is a scaling factor chosen to produce integer
values for the scores~\cite{Henikoff:1992tk}.

Different variants of the BLOSUM matrices exist; for a chosen threshold $L$,
only sequences within a sequence identity threshold of $L\%$ are clustered into
a single representative sequence; those sequences are then aligned and the
alignment used to compute the BLOSUM$L$ matrix.
Thus, BLOSUM80 is intended for use in less divergent sequence alignments,
while BLOSUM50 is intended for use in more divergent sequence alignments.
BLOSUM62 is a commonly used default scoring matrix for protein sequence
alignment tools such as BLAST~\cite{Altschul:1997tl}.

There are newer BLASTP variants,
such as PSI-BLAST~\cite{Altschul:1997tl} and DELTA-BLAST~\cite{Boratyn:2012er} 
that improve
sensitivity by replacing BLOSUM with a \emph{Position-specific scoring matrix}
(PSSM) that scores mismatches differently depending on where they occur in the
alignment. 
PSI-BLAST determines its PSSM by iterative search: First, it performs a
standard BLASTP search, and computes a PSSM from the resulting alignment.
It then repeats this process, searching with the PSSM created by the previous
iteration, and computing a new PSSM.
In contrast, DELTA-BLAST uses pre-determined PSSMs derived from the Conserved
Domains Database (CDD)~\cite{MarchlerBauer:2005uv}, essentially groups of proteins already
determined to be homologous.

BLAST and its derivatives, such as PSI-BLAST and DELTA-BLAST, are effective at
identifying homologous protein sequences for a query sequence when those
homologous sequences share a reasonable amount of sequence identity with the
query sequence~\cite{Rost:1999taa}.
However, we wish to be able to identify homologous proteins--those that share
structural, functional, and evolutionary relationships--even when they do not
share a great deal of sequence similarity.
Since protein structure is more highly conserved than 
sequence~\cite{Dalal:1997wl}, we would like to incorporate
information that is not simply derived from sequence alignments.

\subsection{Structural Alignment}

Just as we can align the sequences of two or more proteins in order to compare
them, we can also align the \emph{structures} of two or more proteins.
Clearly, protein structure alignment requires knowing the tertiary 
structure--the three dimensional coordinates of all the atoms, or at very least
the backbone atoms--of the proteins to be aligned.

Structural alignment can be used to measure structural similarity, and from 
there infer functional and evolutionary relationships.
Structural alignment can also be used to measure the quality of a computational
protein structure \emph{prediction} versus a known, solved structure.
In general, protein structural alignment relies on some form of geometric
superposition, though a wide variety of algorithms exist for efficiently
computing this superposition.
In fact, computing the \emph{optimal} geometric superposition is known to be
NP-hard~\cite{WANG:1994jq}.
Several heuristic approaches have been developed for practical protein
structural alignment.
DALI~\cite{Shindyalov:1998wn}, for example, breaks the structures into hexapeptide fragments and 
calculates a distance matrix by evaluating the contact patterns between
fragments.
DALI then compares these distance matrices and applies a score-maximization
search to compute an alignment.
This approach is called ``aligned fragment pair'' alignment, and is also used by
MAMMOTH~\cite{Ortiz:2009wx}, which instead applies dynamic programming to 
compute the alignment.
Matt~\cite{Menke:2008wu} also uses aligned fragment pairs, but allows ``impossible''
translations and twists in order to better capture structural similarities at
the superfamily or even fold levels.
Hybrid aligners, which use both sequence and structure information, also exist.
DeepAlign~\cite{Wang:2012wq} incorporates not just atomic coordinates but also
secondary structural annotation and sequence information.
Our own Formatt~\cite{Daniels:2012jr} also combines sequence and structure 
information, to try to avoid ``register errors'' that trade significant 
sequence alignment errors for small structural gains.
Most of these methods are fundamentally solving a bi-criterion optimization
problem: we wish to align as much of the input proteins' structure as possible,
while at the same time minimizing the root mean square distance (RMSD) of the
resulting alignment, where RMSD is defined as the square root of the average 
distance between corresponding $\alpha$-carbon atoms between the backbones of 
the proteins in alignment~\cite{Kabsch:1976tp}.

When aligning more distantly-related proteins, structural alignment methods
often outperform purely sequence-based methods~\cite{Chothia:1986tm}.
For this reason, protein structural alignment is routinely used to produce
alignments of homologous proteins to form training sets for remote homology
detection techniques.

\section{Hidden Markov Models}

A Markov model represents a series of observations via a probabilistic 
finite-state automaton.
A Markov model on an alphabet $A$ is a triplet

\begin{equation}
  M=(Q,\pi,\alpha),
\end{equation}  
   where $Q$ is
a finite set of states, each state generates a character from $A$, $\pi$ is the
set of initial state probabilities, and $\alpha$ is the set of state transition
probabilities.
Markov models are so named because they uphold the \emph{Markov property},
which states that future states depend only on the current state of the system.
In other words, first-order Markov models are ``memoryless.''
A Markov model may take into account a \emph{fixed} number of past states (a 
$k_{th}$-order Markov model make take into account $k$ past states).

A hidden Markov model (HMM) represents a series (sometimes a time series) of
observations by a ``hidden'' stochastic process.
HMMs were originally developed for speech recognition~\cite{Viterbi:1967hq, Rabiner:1989vx}.
A HMM is on an alphabet $A$ is a 5-tuple
\begin{equation}
 M=(Q,V,\pi,\alpha,\beta),
 \end{equation}
  where $Q$ is again a finite set
of states, $V$ is a finite set of observations per state, $\pi$ is the set of
initial state probabilities, $\alpha$ is again the set of state transition
probabilities, and $\beta$ is the finite set of emission probabilities over the
alphabet $A$.
Hidden Markov models differ from ordinary Markov models in that,
while the emissions are observable, the states occupied by the finite state 
machine are not themselves observable.
There are three problems to be solved regarding HMMs, and correspondingly, three
algorithms to solve them.

The first problem is: \emph{Given an HMM $M$ and an observed sequence $S$, 
compute the most probable path through $M$ that generates $S$.}

This problem is solved by the \emph{Viterbi algorithm}~\cite{Viterbi:1967hq}, which
is typically implemented using dynamic programming.
The Viterbi algorithm solves the recurrence relation:
\begin{equation}
  \begin{array}{rcl}
  V_{1,k} &=& \mathrm{P}\big( s_1 \ | \ k \big) \cdot \pi_k \\
  V_{t,k} &=& \mathrm{P}\big( s_t \ | \ k \big) \cdot \max_{x \in Q} \left( a_{x,k} \cdot V_{t-1,x}\right)
  \end{array}
\end{equation}

where $V_{t,k}$ is the probability of the most probable state sequence emitting 
the first $t$ observations with $k$ as its final state, and $s_i \in S$ is the
$i^{th}$ observation in $S$. 
The corresponding path through the model can be retrieved by remembering what
series of transitions among states $x \in Q$ were chosen when solving the
recurrence relation.

The second problem is: \emph{Given an HMM $M$ and a sequence of observations 
$S$, compute $P(S|M)$, the probability of observing the sequence $S$ emitted by 
the model $M$.}

This problem is solved by the \emph{forward algorithm}, which relies on dynamic 
programming as well.
In essence, the forward algorithm sums the probabilities over all possible state
paths that can emit $S$.
The recurrence relation for the forward algorithm is nearly identical to that
for the Viterbi algorithm, except that it sums, rather than choosing the
maximum from, the probabilities at each step:

\begin{equation}
  \begin{array}{rcl}
  V_{1,k} &=& \mathrm{P}\big( s_1 \ | \ k \big) \cdot \pi_k \\
  V_{t,k} &=& \mathrm{P}\big( s_t \ | \ k \big) \cdot \sum\limits_{x \in Q} \left( a_{x,k} \cdot V_{t-1,x}\right)
  \end{array}
\end{equation}

The third problem is: \emph{Given a set of sequences of observations, $O$,
and a model $M$, determine the transition probabilities $\alpha$ and emission
probabilities $\beta$ that maximize the $P(O|M)$, the likelihood of observing
the set of sequences given the model.}

Typically, a solution to this problem is \emph{estimated} by the 
\emph{Baum-Welch algorithm}~\cite{Baum:1970tv}, which is
an expectation-maximization algorithm.
A more computationally efficient but less accurate alternative is the Viterbi
Training algorithm (not to be confused with the Viterbi algorithm), also known
as \emph{segmental k-means}~\cite{Rabiner:1989vx}.
A simulated annealing search approach to Baum-Welch can also be used to avoid
local optima~\cite{Baldi:1994vc}.

A further explanation of the above algorithms can be found in \cite{Rabiner:1989vx}.

Despite their origins in the field of speech recognition, hidden Markov models 
have been
used in a variety of areas within the realm of computational biology.
In the context of DNA sequence analysis, HMMs have been 
used~\cite{Dasgupta:2002wj} to detect ``CpG islands,'' regions of the genome where 
cytosine and guanine are predominant and adjacent in sequence.
CpG islands are useful for determining the start of transcription 
sequences--the markers that indicate the regions of the genome that code for
protein sequences.
Hidden Markov models were first used to search for DNA sequences in genome
databases by Churchill~\cite{Churchill:1989wj} in the late 1980s.
Later, Krogh et al.~\cite{Krogh:1994hv} used HMMs to model protein evolution.

\subsection{Profile Hidden Markov Models}

With respect to homology detection, \emph{profile} hidden Markov models have
been popular.
In particular, profile HMMs have been used to model families of protein
sequences, in order to predict whether newly-discovered sequences belong to
those families.
Profile hidden Markov models attempt to represent the evolutionary processes
underlying the differences among closely-related proteins.
In addition, HMM-derived clusterings of proteins have been published, such as
Pfam~\cite{Finn:2006ez}, PROSITE~\cite{Hulo:2006du}, and 
SUPERFAMILY~\cite{Wilson:2007cm}.

HMMER~\cite{Eddy:1998ut} and SAM~\cite{Hughey:1996ub} are two popular software 
tools for
homology detection in proteins (though both are also widely used in
nucleotide sequence analysis, as well).
Much of the work in this dissertation is based on HMMER; we chose it as it is
open-source and more actively maintained.

\newcommand\txprobj[3][]{a#1_{{#2}_{j-1}{#3}_j}}
\newcommand\txprobjj[3][]{a#1_{{#2}_{j-1}{#3}_j}}
\newcommand\alignwidth{\ensuremath C} % number of columns in an alignment
\newcommand\pairedwith[1]{{\pi(#1)}}

HMMER models three types of events that may occur during the evolution of a
protein: \emph{insertion}, \emph{deletion}, and \emph{substitution} of an amino
acid at a particular position.
These three possible events become the three hidden states of the HMM.
Substitution events are modeled using a \emph{match} state, which also 
represents amino acids that are conserved, or have not changed, between 
proteins.
In essence, mutated amino acids can be represented as substitutions
using a substitution matrix, and since the most probable substitution in such a
matrix is the identity function, conserved amino acids can also be represented
using the same matrix.
Insertion and match states are both considered \emph{emission} states, as each
corresponds to the presence of an amino acid at a particular position in a 
protein.
Each emission state comprises a table of emission probabilities: the likelihood
that any particular amino acid will be present (emitted) at that position.
Intuitively, for each match state, the most common amino acid seen in the
training data will be the most probable amino acid in the emission table for
that column of the alignment.

HMMER uses the ``Plan7'' hidden Markov model architecture, which forbids direct
transitions between insertion states and deletion states~\cite{Eddy:1998ut}.
``Plan7'' is a pun on ``Plan9,'' the architecture by Krogh, et
al.~\cite{Krogh:1994hv} that allowed all 9 possible transitions among match,
insert, and delete states;
``Plan7'' gets its name because there are exactly 7 possible transitions into
the states of any column of the alignment used for training.
See Figure~\ref{plan7} for an illustration of the Plan7 architecture.

\begin{figure}[htb!]
\advance\leftskip-0.3in
\begin{center}
  \fbox{\includegraphics[width=4in] {intro/plan7.pdf}}
   \caption[The ``Plan7'' architecture for hidden Markov models, as implemented
   in HMMER.]{The ``Plan7'' architecture for hidden Markov models, as 
   implemented in HMMER.
   Dashed circles indicate \emph{nodes} of the model.
   A node groups a match, insertion, and deletion state, along with the
   emission probabilities for the match and insertion states.
   Note: this diagram simplifies the ``Plan7'' architecture; in reality, begin
   and end nodes are more complex, allowing for entire models to repeat.
   }
   \label{plan7}
 \end{center}
\end{figure}

HMMER trains a profile HMM (using a simulated annealing variant of the 
Baum-Welch algorithm)~\cite{McClure:1996uv} on a \emph{sequence 
profile}, which is an alignment of the protein sequences comprising some 
group--such as a SCOP superfamily or family--of putatively homologous proteins.
This alignment may be a sequence alignment or a structural alignment; in this
work we will focus on profiles derived from structural alignments.

An alignment used for training may of course contain \emph{gaps}.
A~gap in row~2, column~$j$ indicates that as proteins evolved, either 
protein~2 lost its amino acid in position~$j$, or 
other proteins gained an amino acid in position~$j$.
If~column~$j$ contains few gaps, 
it~is considered a \emph{consensus column},
and the few proteins with gaps may have lost amino acids via
\emph{deletions}.
Note that this model is directionless with respect to evolutionary change;
it does not distinguish between a residue being gained or lost over time.
If~column~$j$ contains \emph{mostly} gaps, 
it~is considered a \emph{non-consensus column},
and the few proteins without gaps may have gained amino acids via
\emph{insertions}. 

We refer to the amino acid sequence of a protein whose structure we do not know,
and wish to determine using homology detection, as a \emph{query sequence}.
Homology detection using a hidden Markov model involves \emph{aligning} a query
sequence to a hidden Markov model, or computing a \emph{path} through the model
that maximizes the likelihood of the model emitting the query sequence.
This alignment involves assigning successive amino acids in the query sequence
to successive nodes of the model. For a given node of the model, the match
and deletion states are mutually exclusive, as are the insertion and deletion
states. 
However, it is permissible for a path to assign amino acids to both the match 
and insert states of a node.
In addition, the match state consumes exactly one amino acid from the query
sequence, while the insert state may consume many.
The delete state consumes no amino acids from the query sequence.

Given a hidden Markov model, a protein whose query sequence has a higher 
probability is considered to be more likely to 
be homologous to the proteins in the alignment.
We~write a query sequence as $x_1, \ldots, x_{\scriptscriptstyle N}$,
where each $x_i$~is  an amino acid.
The number of amino acids, $N$, can differ from the number of columns
in the alignment,~\alignwidth. 

A~hidden Markov model carries emission probabilities on some states, and
transition probabilities on all edges between states.
Both the probabilities and the states are determined by the alignment:
\begin{itemize}
\item
For each column~$j$ of the alignment, the hidden Markov model has a
\emph{match state}~$M_j$.
The match state contains a table $e_{M_j}(x)$ which gives the
 probability that a homologous protein has amino acid~$x$ in
 column~$j$.
\item 
For each column~$j$ of the alignment, the hidden Markov model has an
\emph{insertion state}~$I_j$.
The insertion state contains a table $e_{I_j}(x)$ which represents the
probability that a homologous protein has gained amino acid~$x$ by
insertion at column~$j$.
\item
For each column~$j$ of the alignment, the hidden Markov model has a
\emph{deletion state}~$D_j$.
The deletion state represents the probability that a homologous protein
has lost an amino acid by deletion from column~$j$.
\end{itemize}
The probabilities $e_{M_j}(x)$ and $e_{I_j}(x)$ are \emph{emission probabilities}.
Each tuple of match, insertion, and deletion states is called a \emph{node}
of the hidden Markov model.

Each transition has its own probability:
\begin{itemize} 
\item
A~transition into a match state 
is more likely when column~$j$ is a consensus column.
Depending on the predecessor state, 
the probability of such a transition is 
$\txprobj M M$, $\txprobj I M$, or~$\txprobj D M$.
\item
A~transition into a deletion state 
is more likely when column~$j$ is a non-consensus column.
The probability of such a transition is 
$\txprobj M D$~or~$\txprobj D D$.
\item
A~transition into an insertion state 
is more likely when column~$j$ is a non-consensus column.
The probability of such a transition is 
$\txprobjj M I$~or~$\txprobjj I I$.
\end{itemize}


Due to the specific topology of the state-transition graph in the ``Plan7''
architecture, a reformulation of the Viterbi recurrence relations are warranted.
In particular, we need not consider all state transitions that would be possible
given a general topology, and instead, need consider only three possible
transitions at each node, which reduces the search space.
The variant of the Viterbi algorithm adapted for the ``Plan7'' architecture is
given by the recurrence relations:

\newcommand\vsum[2]{#2&{}+{}& #1}

\def\goo{18pt}
\def\gum{14pt}

\def\maxiquad{\hskip 1.2em\relax}
\begin{equation}
\begin{array}{@{}l@{}c@{}l}

V_{j}^{M}(i) &{}={}& \frac{e_{M_{j}}(x_{i})}{q_{x_{i}}} \times \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j-1}^{M}(i - 1) \times a_{M_{j-1}M_{j}}\\
  V_{j-1}^{I}(i - 1) \times a_{I_{j-1}M_{j}}\\
  V_{j-1}^{D}(i - 1) \times a_{D_{j-1}M_{j}}\\
  \end{array} \right.\\\\[\goo]
V_{j}^{I}(i) &=& \log\frac{e_{I_{j}}(x_{i})}{q_{x_{i}}} \times \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j}^{M}(i - 1) \times a_{M_{j}I_{j}}\\
  V_{j}^{I}(i - 1) \times a_{I_{j}I_{j}}\\
  \end{array} \right.\\\\[\gum]
V_{j}^{D}(i) &=& \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j-1}^{M}(i) \times a_{M_{j-1}D_{j}}\\
  V_{j-1}^{D}(i) \times a_{D_{j-1}D_{j}}\\
  \end{array} \right.\\

\end{array}
\end{equation}



% However, these other methods have their own
% limitations.
% Typical threading approaches, such as RAPTOR-X, excel at detecting
% remote homologs based on structural similarity, but have massive computational
% demands.
% Profile-profile HMMs, such as HHPred, build sequence alignment based
% profiles for the query as well as the training data, construct a profile HMM
% from the query alignment as well as the training data, and then align the two
% HMMs with each other.
% Using closely homologous sequences in the query profile
% allows profile-profile HMMs detect more remote homologs accurately, but
% building the query profile requires performing a sequence-based homology
% search, which is still computationally expensive, though not as much so as
% threading approaches.

\section{Other Homology Detection Methods}

\subsection{Threading Methods}

Threading is a methodology by which a query sequence is \emph{threaded} onto
a structural template, and the quality of the threading is evaluated by means
of an energy function or a statistical likelihood.
The idea of threading is based on the observation that the number
of unique protein folds found in nature is small with respect to the number of
distinct protein sequences, and that relatively few novel protein folds have
been found recently~\cite{Pearl:2003wb}.

THREADER~\cite{Jones:1992wx}, the original threading approach, aligns a protein 
sequence to a full tertiary structure model of a protein, and computes a score 
based upon a Boltzmann energy function and solvent potentials.
THREADER thus evaluates the propensity a sequence has for forming a particular
tertiary structure, but it cannot distinguish homologs (evolutionarily related
proteins that share structure and possibly function) from \emph{analogs} 
(proteins that happen to share similar structure but have no evolutionary 
relationship)~\cite{Orengo:1994fh, Jones:1997ve}.

GenTHREADER~\cite{Jones:1999im} improves upon THREADER, using an artificial 
neural network to compute a score
based upon multiple inputs: solvent and Boltzmann potentials like THREADER, but
also a sequence alignment score and length, and the lengths of the query
sequence and template.

Another popular and successful threading method is RAPTOR~\cite{Xu:2003p3417}, 
which relies on a template based on a 
\emph{contact map} to indicate which residues in a protein are in close
geometric proximity to one another, as well as the statistical propensities for
individual residues to be in such proximity.
RAPTOR then relies on linear programming to compute the optimal alignment
of a query sequence to this template, in order to minimize the statistical 
energy.
In Chapter~\ref{chapter:c3_smurflite}, we compare our results for remote
homology detection to those of RAPTOR.

Other threading methods include SPARKS X~\cite{Yang:2011id} and the
recently-developed RaptorX~\cite{Peng:2011wx}.

\subsection{Profile-Profile Hidden Markov Models}

Several recent efforts have improved upon profile hidden Markov models, by
aligning a profile HMM built from a training profile (much like HMMER) with
another profile HMM built from the query sequence.
HHPred~\cite{Soding:2005ff}, MUSTER~\cite{Wu:2008vh}, and HHblits~\cite{Remmert:2012cj} are three such
approaches.
Given a query sequence, HHPred relies on PSI-BLAST~\cite{Altschul:1997tl} to build a sequence
profile.
HHPred then builds a profile HMM on this profile, and uses a variant of the
Viterbi algorithm to align the \emph{query} HMM to candidate \emph{target} HMMs.
HHPred, along with other profile-profile hidden Markov model methods, relies on
the query profile to more faithfully represent the evolutionary variation in the
protein sequences that may be homologous to the query sequence.
In Chapter~\ref{chapter:c3_smurflite}, we compare our results for remote 
homology detection to those of HHPred.


\subsection{Markov random fields}


Some researchers have suggested generalizing HMMs to the more powerful
Markov random fields (MRFs).
Unlike HMMs, which model only local dependencies among neighboring residues,
MRFs can capture nonlocal interactions, such as the conservation of
hydrogen-bonded residues in paired $\beta$-strands.
SMURF (Structural Motifs Using
Random Fields)~\cite{Menke:2009, Menke:2010ti} used this $\beta$-strand 
information to recognize remote homologs
in the $\beta$-propeller folds better than HMM methods.
However, SMURF is limited by computational complexity,
because it uses multidimensional dynamic programming to
compute an optimal parse of a query sequence onto the MRF, and its
computational complexity is exponential in something called the interleave
number of a structure.
This interleave number is simply the number of
intervening $\beta$-strands (in sequence) between a pair of hydrogen-bonded,
paired $\beta$-strands.
$\beta$-propellers have a maximum interleave number of three,
and thus they are tractable for SMURF.
In contrast, some $\beta$-barrels and
sandwiches have an interleave number as high as 12, and thus, SMURF's
computational complexity becomes intractable on available computer systems.
Chapters~\ref{chapter:c3_smurflite}~and~\ref{chapter:c4_mrfy} explore two
alternative approaches to mitigate this
computational hindrance.




\section{Remote Homology Detection} 
% \remark{Lenore wanted this earlier, before
% HMMs, but it is unclear how to introduce it. I like introducing it based upon
% limitations of existing methods. I also like how this leads into the Touring
% chapter.}

While computing tertiary structure is computationally challenging, we can take
comfort in the fact that we do not always need tertiary structure to make useful
predictions as to function or evolutionary similarity.
In particular, supersecondary structure is often enough.
Since structure determines function, if we can classify a new protein of
unknown structure as sharing similar supersecondary structure to a group of
known proteins, we have evidence that the new protein shares a similar function
to those known proteins.


Protein sequence is much typically less conserved than
structure~\cite{Kolodny:2006jv, Wilson:2000ed}, so proteins 
of similar
structure and function, as seen at the SCOP superfamily level, may lack any
meaningful sequence similarity.
Simple sequence comparisons such as BLAST fail
to correctly identify these remote homologs.
However, if biologists sequence
the genome of an organism, they will wish to functionally annotate the proteins
coded for by its genes.
Anton or Folding@Home would require weeks or months of
computational time per gene to compute tertiary structure, and even a bacterium
has thousands of genes.
Supersecondary structure provides enough information to
make reasonable functional annotations, and we can compute it quickly enough to
scale to entire genomes.
Even threading approaches such as RAPTOR\cite{Xu:2003p3417} may require hours per
gene.
The methods we have developed are faster and more accurate than standard
threading approaches.


Threading methods such as RAPTOR~\cite{Xu:2003p3417} attempt to map a new protein sequence
onto templates built from individual solved proteins.
While a high-quality
threading hit may produce an accurate tertiary structure, this approach loses
the ability to take a larger evolutionary view of protein space.
Profile-based
methods--including SMURF--build knowledge about evolutionarily conserved
parts of the protein structure and sequence into their templates.
Rather than
matching a new protein sequence to a single best-fitting structure, we wish to
say that a new sequence belongs to a group of proteins that share evolutionary,
structural, and possibly functional similarities.


In order to predict that a new protein sequence shares structure and function
with a group of known proteins, we must be sure that these groups of proteins
are consistent.
In particular, since we use structure to infer function, we
wish to ensure that protein space is organized in a structurally consistent way.


\section{Outline of This Work}

In this dissertation, we present several approaches to remedying the complexity
of MRFs for remote homology detection, as well as an approach to improving the
quality of training data for remote homology detection.
Below is the outline of individual chapters in this dissertation.

We begin with a tour of
protein fold space (Chapter~\ref{chapter:c2_touring}), examining the structural
consistency of the SCOP protein structural hierarchy.
We also introduce a method for clustering protein structures such that 
manually-curated hierarchies such as SCOP~\cite{Murzin:1995uh} can be recreated with 
reasonable accuracy, based purely on automated structural alignments.
We also introduce a benchmark set, called MattBench, that we propose for use by
the developers of protein sequence or structural aligners.


In Chapter~\ref{chapter:c3_smurflite}, we discuss an approach to generalizing
Markov random fields to
the problem of remote homology detection in $\beta$-structural proteins.
We simplify the SMURF~\cite{Menke:2009, Menke:2010ti} 
Markov random field model by limiting the complexity of the
dependency graph, in order to bound the computational complexity of finding
an optimal parse of a query sequence to a model.
We combine these simplified Markov random fields with a model of ``simulated 
evolution'' to improve upon existing methods.

In Chapter~\ref{chapter:c4_mrfy}, we introduce an approach for remote homology
detection using the SMURF Markov random fields that does not require 
simplifying the dependency graph.
Instead, we introduce a stochastic search approach that quickly computes
approximate alignments to the Markov random field, and which should be 
generalizable to all protein folds.

Finally, in Chapter 5, we discuss the results and summarize the key findings of 
this dissertation followed by possible directions for future work.  


